{
	"Meta(Acqusition)": {
		"doc_id": "REPORT-news_r-00333",
		"doc_category": "REPORT",
		"doc_type": "news_r",
		"doc_name": "[고학수의 미래를 묻다] 인공지능은 차별·편견을 인간에게서 배운다",
		"author": "김선영",
		"publisher": "중앙일보",
		"publisher_year": "2020",
		"doc_origin": "중앙일보"
	},
	"Meta(Refine)": {
		"passage_id": "REPORT-news_r-00333-00002",
		"passage": " 인공지능은 어떻게 판단할까.\n  과거에 여성이 경제 활동을 장기적으로 지속하기 쉽지 않았던 사회였다면, 인공지능은 이를 반영해 여성에게 더 불리한 조건을 매길 수 있다.\n  지금까지의 경제 활동이 비슷하더라도 10, 20년 후에는 남녀 사이에 격차가 벌어질 것이라는 통계적 경험이 이런 결과를 가져오는 것이다.\n  실제 지난해 말 미국에서 신용카드 발급을 둘러싸고 차별 주장이 제기됐다.\n  이른바 ‘애플 카드’ 사건이다.\n  애플이 골드만삭스와 협력해 내놓은 신용카드인데, 소득·자산 등의 여건이 똑같아도 인공지능은 남성에게 훨씬 큰 카드 사용 한도를 부여한 경우가 있었다.\n  심지어 애플 공동창업자인 스티브 워즈니악도 방송에 나와 불평했다.\n  “아내와 나는 재산과 금융계좌를 모두 공동 소유한다.\n  그런데 내 카드 한도가 아내의 10배다.\n ” 해외에서 인공지능은 법 절차에 활용되기도 한다.\n  가석방 여부를 결정할 때, 출소 후 다시 범죄를 저지를 가능성에 대해 인공지능의 도움을 받아 판단하는 것이 하나의 예다.\n  여기서도 문제가 드러났다.\n  인공지능이 재범률을 제대로 예측했는지, 사후에 검증한 탐사보도가 미국에서 있었다.\n  백인은 인공지능이 예상한 것보다 가석방 후 재범률이 높은 경우가 많았고, 흑인은 그 반대였다.\n  무엇이 문제였을까.\n  인공지능이 판단하는 과정을 따라가 보자.\n  인공지능은 가석방 후 다시 범죄를 저지른 사람과 그렇지 않았던 사람들의 데이터를 학습한다.\n  두 유형 각각의 특징을 섬세하게 살펴서는, 이를 바탕으로 ‘재범 가능성’이라는 미래 확률을 예측하게 된다.\n  그런데 이렇게 인공지능이 학습하는 데이터가 부적절하다.\n  누가 몇 번이나 범죄 행위를 했는지 알려주는 ‘정확한 재범률 데이터’란 것은 어디에도 없기 때문이다.\n  범죄가 발생해도 정작 범인이 잡힐 때까지는 누가 저지른 것인지 알 수 없다.\n  화성 연쇄살인 사건을 저지른 이춘재가 좋은 사례다.\n  처제를 살해해 복역 중이던 이춘재가 자백하기 전에는 그가 얼마나 많은 살인을 행했는지 아무도 모르지 않았나.\n  자백 전까지 그의 재범률 데이터는 의미 없는 것이었다.\n  이런 데이터로 재범 가능성을 평가하는 것은 결국 왜곡된 판단을 부를 가능성이 크다.\n  안면 인식, 가석방, 채용과 대출 결정에 이르기까지 인공지능은 이렇게 종종 잘못된 판단을 한다.\n  이유가 뭘까.\n  반쯤은 답이 나왔다.\n  데이터와 알고리즘이 문제다.\n ",
		"passage_Cnt": 1205
	},
	"Annotation": {
		"summary1": "여성의 경제 활동이 인공지능으로 인해 불리해질 수 있으며 실제 미국에서 발급한 애플 카드가 여성보다 남성에게 큰 카드 사용 한도를 부여했다.",
		"summary2": null,
		"summary3": "과거에 여성이 경제 활동을 장기적으로 지속하기 쉽지 않았던 사회였다면, 인공지능은 이를 반영해 여성에게 더 불리한 조건을 매길 수 있다. 실제 지난해 말 미국에서 신용카드 발급을 둘러싸고 차별 주장이 제기됐다. 이른바 ‘애플 카드’ 사건이다. 애플이 골드만삭스와 협력해 내놓은 신용카드인데, 소득·자산 등의 여건이 똑같아도 인공지능은 남성에게 훨씬 큰 카드 사용 한도를 부여한 경우가 있었다.",
		"summary_3_cnt": 222
	}
}